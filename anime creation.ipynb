{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anime Generation\n",
    "## DCGAN \n",
    "\n",
    "> Data pre-processing [here](https://www.tensorflow.org/tutorials/load_data/images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "from IPython import display\n",
    "the_voice = './sounds/chose_a_voice.wav'\n",
    "the_creation = './sounds/You_created_me.wav'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "# Error: Node: 'sequential_3/dropout_4/dropout/random_uniform/RandomUniform'\n",
    "# OOM when allocating tensor with shape[256,180,120,32] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
    "# \t [[{{node sequential_3/dropout_4/dropout/random_uniform/RandomUniform}}]]\n",
    "# the solution to thi is report_tensor_allocations_upon_oom = True\n",
    "\n",
    "import os\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "\n",
    "run_opts = tf.compat.v1.RunOptions(report_tensor_allocations_upon_oom=True)\n",
    "runmeta = tf.compat.v1.RunMetadata()\n",
    "tf.distribute.experimental.MultiWorkerMirroredStrategy(\n",
    "    communication=tf.distribute.experimental.CollectiveCommunication.AUTO,\n",
    "    cluster_resolver=None\n",
    ")\n",
    "\n",
    "\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import imageio\n",
    "import tensorflow_docs\n",
    "import PIL\n",
    "import time\n",
    "import wandb\n",
    "from train.dcgan import DCGAN\n",
    "# pip install wandb\n",
    "# wandb login\n",
    "# 65bacd21c40b0085e299e05ea94b552d5119b7bc\n",
    "\n",
    "wandb.init(project=\"Anime Creation\", entity=\"skycladai\")\n",
    "\n",
    "print(\"Note! You will need tensorflow-gpu version 2.8.0 or higher. Your version is\", tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "Audio(url=the_voice, autoplay=True, rate=48000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE = 254 # or 64 this cause memory allocation failure\n",
    "BATCH_SIZE = 8\n",
    "IMAGE_WIDTH = 240\n",
    "IMAGE_LENGTH = 360\n",
    "ORIGINAL_IMAGE_SIZE = (360, 240)\n",
    "RGB = 3\n",
    "BUFFER_SIZE = 58085\n",
    "EPOCHS = 6\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 9\n",
    "wandb.config = {\n",
    "  \"learning_rate\": 0.001,\n",
    "  \"epochs\": EPOCHS,\n",
    "  \"batch_size\": BATCH_SIZE\n",
    "}\n",
    "DCGAN(batch_size= BATCH_SIZE, noise_dim= noise_dim, num_examples_to_generate=num_examples_to_generate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "x_train = tf.keras.preprocessing.image_dataset_from_directory( directory=\"./dataset/\", validation_split=0.3, subset=\"training\" ,label_mode=None, batch_size=BATCH_SIZE, image_size=ORIGINAL_IMAGE_SIZE, seed=123 , shuffle=BUFFER_SIZE)\n",
    "x_test = tf.keras.preprocessing.image_dataset_from_directory( directory=\"./dataset/\", validation_split=0.3, subset=\"validation\" ,label_mode=None, batch_size=BATCH_SIZE, image_size=ORIGINAL_IMAGE_SIZE, seed=123, shuffle=BUFFER_SIZE)\n",
    "    \n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title(\"Training Images\")\n",
    "for images in x_train.take(1):\n",
    "  for i in range(6):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "    plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_batch in x_train:\n",
    "    print(image_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "# Batch and shuffle the data\n",
    "def configure_for_performance(ds, name):\n",
    "    ds = ds.cache(name)\n",
    "    ds = ds.shuffle(buffer_size=2000)\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "normalized_ds = x_train.map(lambda x: (normalization_layer(x)))\n",
    "image_batch = next(iter(normalized_ds))\n",
    "# Cache keeps the images in memory after they're loaded off disk during the first epoch. \n",
    "# This will ensure the dataset does not become a bottleneck while training your model. \n",
    "# Prefetch overlaps data preprocessing and model execution while training.\n",
    "# This allows later elements to be prepared while the current element is being processed. \n",
    "# This often improves latency and throughput, at the cost of using additional memory to store prefetched elements.\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "print(normalized_ds)\n",
    "\n",
    "train_ds = configure_for_performance(normalized_ds, \"./cache/training_cashe\")\n",
    "val_ds = configure_for_performance(x_test, \"./cache/testing_cashe\")\n",
    "for image_batch in train_ds:\n",
    "    print(image_batch.shape)\n",
    "    break\n",
    "\n",
    "print(\"In range between: \", np.min(image_batch[0]),\" and: \", np.max(image_batch[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DCGAN \n",
    "> Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "noise = tf.random.normal([1, 100])\n",
    "generated_image = DCGAN.generator(noise, training=False)\n",
    "print(generated_image.shape)\n",
    "plt.imshow(generated_image[0, :, :, 0])\n",
    "DCGAN.generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Discriminator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision = DCGAN.discriminator(generated_image)\n",
    "print(decision)\n",
    "DCGAN.discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Save Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training DCGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Training definitions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Actual Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "DCGAN.train(train_ds, EPOCHS)\n",
    "\n",
    "Audio(url=the_creation, autoplay=True, rate=48000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DCGAN.checkpoint.restore(tf.train.latest_checkpoint(DCGAN.checkpoint_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > Create a GIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Display a single image using the epoch number\n",
    "def display_image(epoch_no):\n",
    "  return PIL.Image.open('./generated_images/image_at_epoch_{:04d}.png'.format(epoch_no))\n",
    "display_image(6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim_file = 'dcgan.gif'\n",
    "\n",
    "with imageio.get_writer(anim_file, mode='I') as writer:\n",
    "  filenames = glob.glob('image*.png')\n",
    "  filenames = sorted(filenames)\n",
    "  for filename in filenames:\n",
    "    image = imageio.imread(filename)\n",
    "    writer.append_data(image)\n",
    "  image = imageio.imread(filename)\n",
    "  writer.append_data(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_docs.vis.embed as embed\n",
    "embed.embed_file(anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7139d3ec96369264d417f4ab347d9b038e4df9abf4bf8a696401e6bfea45ed2c"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
