{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anime Generation\n",
    "## DCGAN \n",
    "\n",
    "> Data pre-processing [here](https://www.tensorflow.org/tutorials/load_data/images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "# trainlets < version 5.0.0\n",
    "# os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "# os.environ['WANDB_NOTEBOOK_NAME'] = 'Anime Faces and Names'\n",
    "# os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "# trainlets > version 5.0.0 required for tensorflow 2.2.1 and python 3.9\n",
    "os.environ.get('TF_GPU_ALLOCATOR', 'cuda_malloc_async')\n",
    "# os.environ.get('WANDB_NOTEBOOK_NAME', \"Anime Faces and Names\")\n",
    "os.environ.get('TF_CPP_MIN_LOG_LEVEL', '2')\n",
    "\n",
    "import winsound\n",
    "from IPython import display\n",
    "the_voice = \"./sounds/chose_a_voice.wav\"\n",
    "the_creation = \"./sounds/You_created_me.wav\"\n",
    "\n",
    "import tensorflow as tf\n",
    "# Error: Node: 'sequential_3/dropout_4/dropout/random_uniform/RandomUniform'\n",
    "# OOM when allocating tensor with shape[256,180,120,32] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
    "# \t [[{{node sequential_3/dropout_4/dropout/random_uniform/RandomUniform}}]]\n",
    "# the solution to thi is report_tensor_allocations_upon_oom = True\n",
    "# Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. \n",
    "# This isn't available when running in Eager mode.\n",
    "# In this project we are running in eager mode!\n",
    "# you can check this by running the following code: tf.executing_eagerly()\n",
    "# run_opts = tf.compat.v1.RunOptions(report_tensor_allocations_upon_oom=True)\n",
    "# print(\"run_opts : => \", run_opts)\n",
    "\n",
    "# runmeta = tf.compat.v1.RunMetadata()\n",
    "# physical_devices = tf.config.list_physical_devices(\"GPU\")\n",
    "# print(\"physical_devices : => \", physical_devices)\n",
    "# tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "# tf.distribute.MultiWorkerMirroredStrategy(\n",
    "#     cluster_resolver=None, communication_options=None\n",
    "# )\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, preprocessing\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imageio\n",
    "import tensorflow_docs\n",
    "import PIL\n",
    "\n",
    "print(\"Note! You will need tensorflow-gpu version 2.8.0 or higher. Your version is\", tf.__version__)\n",
    "\n",
    "winsound.PlaySound(sound=the_voice, flags=winsound.SND_ALIAS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE = 254 # or 64 this cause memory allocation failure\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 2000\n",
    "IMAGE_WIDTH = 240\n",
    "IMAGE_LENGTH = 360\n",
    "ORIGINAL_IMAGE_SIZE = (360, 240)\n",
    "EPOCHS = 15000\n",
    "CURRENT_EPOCH = 3000\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 9\n",
    "learning_rate=0.0002\n",
    "# data_directory= \"./dataset_faces/\"\n",
    "data_directory= \"./dataset/\"\n",
    "anim_file = \"anime_characters_dcgan.gif\"\n",
    "# folder = './anime_faces/'\n",
    "folder = \"./new_anime_chars/\"\n",
    "new_images_root = folder + \"generated_images/\"\n",
    "training_checkpoints_root = folder + \"training_checkpoints/\"\n",
    "\n",
    "# global variables\n",
    "global genereator_loss_list, discriminator_loss_list \n",
    "genereator_loss_list, discriminator_loss_list = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "x_train = preprocessing.image_dataset_from_directory( \n",
    "  directory= data_directory, \n",
    "  validation_split=0.5,\n",
    "  subset='training',\n",
    "  label_mode=None, \n",
    "  batch_size=BATCH_SIZE, \n",
    "  image_size=ORIGINAL_IMAGE_SIZE, \n",
    "  seed=123 , \n",
    "shuffle=True)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title(\"Training Images\")\n",
    "for images in x_train.take(1):\n",
    "  print(images.shape)\n",
    "  for i in range(6):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "    plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Cashe Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Wall time: 5min 22s\")\n",
    "from os.path import exists\n",
    "\n",
    "file_exists = exists('lost_list.csv') and os.stat('lost_list.csv').st_size == 0\n",
    "if file_exists:\n",
    "    data_loss = pd.read_csv('lost_list.csv')\n",
    "    global genereator_loss_list, discriminator_loss_list\n",
    "    genereator_loss_list, discriminator_loss_list = data_loss['genereator_loss_list'].to_numpy().tolist(), data_loss['discriminator_loss_list'].to_numpy()\n",
    "    \n",
    "# print(\"genereator_loss_list = \", genereator_loss_list)\n",
    "# print(\"discriminator_loss_list = \", discriminator_loss_list)\n",
    "\n",
    "# Cache keeps the images in memory after they're loaded off disk during the first epoch. \n",
    "# This will ensure the dataset does not become a bottleneck while training your model. \n",
    "# Prefetch overlaps data preprocessing and model execution while training.\n",
    "# This allows later elements to be prepared while the current element is being processed. \n",
    "# This often improves latency and throughput, at the cost of using additional memory to store prefetched elements.\n",
    "def configure_for_performance(ds, name):\n",
    "    ds = ds.cache(name)\n",
    "    # ds = ds.shuffle(buffer_size=2000) # we are making shuffle at the loading\n",
    "    ds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "# normalized_ds = x_train.map(tf.keras.layers.Rescaling(scale=1./255)) # scale 0 to 1\n",
    "normalized_ds = x_train.map(tf.keras.layers.Rescaling(scale=1./127.5, offset=-1)) # scale -1 to 1 because we are using tanh\n",
    "# Cashe, shuffle and prefetch the data\n",
    "train_ds = configure_for_performance(normalized_ds, \"./cache/training_cashe\")\n",
    "\n",
    "for image_batch in train_ds:\n",
    "    print(image_batch.shape)\n",
    "    print(\"In range between: \", np.min(image_batch[0]),\" and: \", np.max(image_batch[0]))\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DCGAN \n",
    "> Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_model():\n",
    "    print(\"Generating DCGAN model.\")\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(45*30*256, use_bias=False, input_shape=(100,)))\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    model.add(layers.Reshape((45, 30, 256)))\n",
    "    assert model.output_shape == (None, 45, 30, 256)  # Note: None is the batch size\n",
    "    \n",
    "    model.add(layers.Conv2DTranspose(256, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 45, 30, 256)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 90, 60, 128)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    # Add another Conv2DTranspose to convert the image to the original dimensions\n",
    "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 90, 60, 128)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 180, 120, 64)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
    "    assert model.output_shape == (None, 360, 240, 3)\n",
    "\n",
    "    print(\"DCGAN model completed\")\n",
    "    return model\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "generator = generator_model()\n",
    "generated_image = generator(noise, training=False)\n",
    "print(generated_image.shape)\n",
    "plt.imshow(generated_image[0, :, :, 0]) # default RGB\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Discriminator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_model():\n",
    "    print(\"DCGAN descriminator model\")\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
    "                                    input_shape=[360, 240, 3]))\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    \n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D(pool_size=(5, 5)))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(1, 1), padding='same'))\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    \n",
    "    model.add(layers.Conv2D(256, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D(pool_size=(5, 5)))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(256))\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Dense(256))\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Dense(1))\n",
    "    \n",
    "    print(\"DCGAN descriminator Completed!!\")\n",
    "    return model\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = discriminator_model()\n",
    "decision = discriminator(generated_image)\n",
    "# print(decision)\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and plot images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and Checkpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_images(folder):\n",
    "    filenames = glob.glob(new_images_root + folder + 'image*.png')\n",
    "    return sorted(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "    # This is so all layers run in inference mode (batchnorm).\n",
    "    predictions = model(test_input, training=False)\n",
    "    # predictions = ((predictions + 1) /2)*255\n",
    "    \n",
    "    for i in range(predictions.shape[0]):\n",
    "        img = tf.keras.preprocessing.image.array_to_img(\n",
    "                (predictions[i] + 1) * 127.5, scale=False\n",
    "                )\n",
    "        name = 'image_at_epoch_{:04d}.png'.format(i)\n",
    "        img.save(os.path.join(new_images_root + 'steps', name))\n",
    "    \n",
    "    file_images = read_images('steps/')\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    i=0\n",
    "    for imgs in file_images:\n",
    "        image = imageio.imread(imgs)\n",
    "        plt.subplot(3, 3, i+1)\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "        i+=1\n",
    "    plt.savefig(new_images_root + 'image_at_epoch_{:04d}.png'.format(epoch))\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])\n",
    "\n",
    "generator_optimizer = Adam(learning_rate)\n",
    "discriminator_optimizer = Adam(learning_rate)\n",
    "\n",
    "checkpoint_dir = training_checkpoints_root\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(\n",
    "    generator_optimizer=generator_optimizer,\n",
    "    discriminator_optimizer=discriminator_optimizer,\n",
    "    generator=generator,\n",
    "    discriminator=discriminator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Plot loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT\n",
    "def plot_loss(gen_loss, disc_loss):\n",
    "    x = np.arange(0, len(gen_loss) )\n",
    "\n",
    "    # plotting\n",
    "    fig_gen = plt.figure(figsize=(10, 6), dpi=80)\n",
    "    plt.title(\"Generator Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.plot(x, gen_loss, color =\"green\")\n",
    "    plt.show()\n",
    "    fig_gen.savefig(new_images_root + 'gen_loss_plot.png')\n",
    "    plt.close()\n",
    "\n",
    "    fig_disc = plt.figure(figsize=(10, 6), dpi=80)\n",
    "    plt.title(\"Discriminator Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.plot(x, disc_loss, color =\"green\")\n",
    "    plt.show()\n",
    "    fig_disc.savefig(new_images_root + 'disc_loss_plot.png')\n",
    "    plt.close()\n",
    "\n",
    "    fig_gen_disc = plt.figure(figsize=(10, 6), dpi=80)\n",
    "    plt.title(\"Generator and Discriminator Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.plot(x, gen_loss, color =\"green\")\n",
    "    plt.plot(x, disc_loss, color =\"red\")\n",
    "    plt.show()\n",
    "    fig_gen_disc.savefig(new_images_root + 'loss_plot.png')\n",
    "    plt.close()\n",
    "\n",
    "def save_loss_csv(genereator_loss, discriminator_loss):\n",
    "    data_loss = pd.DataFrame({\"genereator_loss_list\" : genereator_loss, \"discriminator_loss_list\" : discriminator_loss})\n",
    "    data_loss.to_csv(\"lost_list.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training DCGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training definitions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "    \n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "\n",
    "        generated_images = generator(noise, training=True)\n",
    "        real_out = discriminator(images, training=True)\n",
    "        fake_out = discriminator(generated_images, training=True)\n",
    "        \n",
    "        gen_loss = generator_loss(fake_out)\n",
    "        \n",
    "        disc_loss = discriminator_loss(real_output=real_out, fake_output=fake_out)\n",
    "        \n",
    "        gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "        gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "        generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "        discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "        \n",
    "        return gen_loss, disc_loss, generated_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "    print(\"Training Start...\")\n",
    "    global genereator_loss_list, discriminator_loss_list\n",
    "    \n",
    "    for epoch in range(CURRENT_EPOCH, epochs):\n",
    "        print(\"Epoch :\", epoch, \"/\", EPOCHS)\n",
    "        start = time.time()\n",
    "        start_step_time = start\n",
    "\n",
    "        for step, image_batch in enumerate(dataset):\n",
    "            gen_loss, disc_loss, generated_image = train_step(image_batch)\n",
    "            if step % 100 == 0:\n",
    "                print('step: %d - gen_loss: %.4f - disc_loss: %.4f - %.3f s' % (step, gen_loss, disc_loss, time.time()-start_step_time))\n",
    "                start_step_time = time.time()\n",
    "        \n",
    "        genereator_loss_list = np.append(genereator_loss_list, gen_loss.numpy())\n",
    "        discriminator_loss_list = np.append(discriminator_loss_list, disc_loss.numpy())\n",
    "        \n",
    "        display.clear_output(wait=True)\n",
    "        # Save the model and image every 5 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            # Produce images for the GIF as you go\n",
    "            generate_and_save_images(generator, epoch + 1, seed)\n",
    "            save_loss_csv(genereator_loss_list, discriminator_loss_list)\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "            plot_loss(genereator_loss_list, discriminator_loss_list)\n",
    "            winsound.Beep(frequency=1000, duration=500 )\n",
    "\n",
    "        print ('Time for epoch {} is {} sec'.format(epoch, time.time()-start))\n",
    "\n",
    "    # Generate image after the final epoch\n",
    "    display.clear_output(wait=True)\n",
    "    generate_and_save_images(generator, epochs, seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "train(dataset=train_ds, epochs=EPOCHS)\n",
    "\n",
    "winsound.PlaySound(sound=the_creation, flags=winsound.SND_ALIAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Create a GIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Display a single image using the epoch number\n",
    "def display_image(epoch_no):\n",
    "  return PIL.Image.open(new_images_root + 'image_at_epoch_{:04d}.png'.format(epoch_no))\n",
    "display_image(EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with imageio.get_writer(anim_file, mode='I') as writer:\n",
    "  filenames = read_images('')\n",
    "  for filename in filenames:\n",
    "    image = imageio.imread(filename)\n",
    "    writer.append_data(image)\n",
    "    # Added this to make the gif more \"proper\"\n",
    "    image = imageio.imread(filename)\n",
    "    writer.append_data(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(discriminator_loss_list)\n",
    "print(genereator_loss_list)\n",
    "\n",
    "save_loss_csv(genereator_loss_list, discriminator_loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(genereator_loss_list, discriminator_loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c0bc1039280fb37a29f38b838bbcc5161984a57887618d99b096f0354ba8a421"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('gpuEnv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
